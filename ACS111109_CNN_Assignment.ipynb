{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a089edee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Data Augmentation (Fixed for Autograder)\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_loader = train_datagen.flow(x_train, y_train, batch_size=64)\n",
    "test_loader = test_datagen.flow(x_test, y_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f41d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Load and Preprocess CIFAR-10\n",
    "(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "train_gen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "test_gen = ImageDataGenerator(rescale=1./255)\n",
    "train_loader = train_gen.flow(x_train, y_train, batch_size=64)\n",
    "test_loader = test_gen.flow(x_test, y_test, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bb6dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Data Augmentation\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "train_gen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "test_gen = ImageDataGenerator(rescale=1./255)\n",
    "train_loader = train_gen.flow(x_train, y_train, batch_size=64)\n",
    "test_loader = test_gen.flow(x_test, y_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d4f31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Model Architecture Enhancement\n",
    "from tensorflow.keras import models, layers\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac6c0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Hyperparameter Optimization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "model.compile(optimizer=Adam(learning_rate=0.0005),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(train_loader, epochs=10, validation_data=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa6b10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "predictions = model.predict(x_test / 255.0)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Acc')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a76d8e",
   "metadata": {},
   "source": [
    "# Task 5: Report / Conclusion\n",
    "本次實驗中，我們針對 CNN 模型進行了多項優化：\n",
    "\n",
    "- 調整模型架構：增加了更多卷積層與 Dropout 強化泛化能力。\n",
    "- 使用 Adam Optimizer 並調整學習率，提升模型穩定性與收斂速度。\n",
    "- 套用資料增強技術（旋轉、平移、翻轉）以擴增訓練樣本多樣性。\n",
    "- 成功繪製模型準確率與損失曲線圖，並取得預測結果。\n",
    "\n",
    "整體而言，本次訓練結果顯示準確率達到預期目標。未來可嘗試引入 Transfer Learning 或更深層架構以進一步優化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728b758c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Build Improved CNN Model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)),\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7857dc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Compile Model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de37be80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5: Train Model\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "history = model.fit(train_loader, validation_data=test_loader,\n",
    "                    epochs=25, callbacks=[early_stop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3183d808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6: Plot Accuracy and Loss\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Acc', marker='o')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Acc', marker='s')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Acc')\n",
    "plt.legend(); plt.grid(True)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss', marker='o')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss', marker='s')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Loss')\n",
    "plt.legend(); plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c3701a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 7: Evaluate\n",
    "loss, acc = model.evaluate(test_loader)\n",
    "print(f\"Test Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e60ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 8: Confusion Matrix\n",
    "y_probs = model.predict(x_test / 255.0)\n",
    "y_preds = np.argmax(y_probs, axis=1)\n",
    "y_true = y_test.flatten()\n",
    "cm = confusion_matrix(y_true, y_preds)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names, cmap='Blues')\n",
    "plt.xlabel('Predicted'); plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286cad97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6: Model Evaluation\n",
    "test_loss, test_acc = model.evaluate(test_loader)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}, Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bb33b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 7: Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "y_true = y_test.flatten()\n",
    "conf_mat = confusion_matrix(y_true, predicted_classes)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ae7077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 8: Display Misclassified Images\n",
    "misclassified_indices = np.where(predicted_classes != y_true)[0]\n",
    "plt.figure(figsize=(12, 12))\n",
    "for i, idx in enumerate(misclassified_indices[:9]):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(x_test[idx])\n",
    "    plt.title(f\"True: {class_names[y_true[idx]]}\\nPred: {class_names[predicted_classes[idx]]}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa664e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 9: Classification Report\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(y_true, predicted_classes, target_names=class_names)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ade9956",
   "metadata": {},
   "source": [
    "# Task 10: Final Summary\n",
    "這次的功課，我自己從頭開始建立了一個 CNN 模型來分類 CIFAR-10 的圖片。\n",
    "\n",
    "- 我加了幾層卷積層和 Dropout，讓模型不會太容易記死（減少過擬合）\n",
    "- 用了資料增強的技巧，像是圖片旋轉、左右翻轉，讓訓練資料更豐富\n",
    "- 在訓練過程中，我畫出了準確率和損失的變化圖，也看了模型預測錯的圖片\n",
    "- 最後的測試準確率還不錯，還有混淆矩陣和分類報告可以幫助分析模型表現\n",
    "\n",
    "如果未來要再提升，我覺得可以考慮使用預訓練模型（像 ResNet）來強化準確率。"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
