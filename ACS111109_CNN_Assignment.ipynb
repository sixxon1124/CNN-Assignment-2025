{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a089edee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Import Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models, regularizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f41d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Load and Preprocess CIFAR-10\n",
    "(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "train_gen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "test_gen = ImageDataGenerator(rescale=1./255)\n",
    "train_loader = train_gen.flow(x_train, y_train, batch_size=64)\n",
    "test_loader = test_gen.flow(x_test, y_test, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bb6dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Data Augmentation\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "train_gen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "test_gen = ImageDataGenerator(rescale=1./255)\n",
    "train_loader = train_gen.flow(x_train, y_train, batch_size=64)\n",
    "test_loader = test_gen.flow(x_test, y_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d4f31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Model Architecture Enhancement\n",
    "from tensorflow.keras import models, layers\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac6c0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Hyperparameter Optimization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "model.compile(optimizer=Adam(learning_rate=0.0005),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(train_loader, epochs=10, validation_data=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa6b10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "predictions = model.predict(x_test / 255.0)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Acc')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a76d8e",
   "metadata": {},
   "source": [
    "# Task 5: Report / Conclusion\n",
    "本次實驗中，我們針對 CNN 模型進行了多項優化：\n",
    "\n",
    "- 調整模型架構：增加了更多卷積層與 Dropout 強化泛化能力。\n",
    "- 使用 Adam Optimizer 並調整學習率，提升模型穩定性與收斂速度。\n",
    "- 套用資料增強技術（旋轉、平移、翻轉）以擴增訓練樣本多樣性。\n",
    "- 成功繪製模型準確率與損失曲線圖，並取得預測結果。\n",
    "\n",
    "整體而言，本次訓練結果顯示準確率達到預期目標。未來可嘗試引入 Transfer Learning 或更深層架構以進一步優化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728b758c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Build Improved CNN Model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)),\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7857dc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Compile Model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de37be80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5: Train Model\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "history = model.fit(train_loader, validation_data=test_loader,\n",
    "                    epochs=25, callbacks=[early_stop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3183d808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6: Plot Accuracy and Loss\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Acc', marker='o')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Acc', marker='s')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Acc')\n",
    "plt.legend(); plt.grid(True)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss', marker='o')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss', marker='s')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Loss')\n",
    "plt.legend(); plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c3701a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 7: Evaluate\n",
    "loss, acc = model.evaluate(test_loader)\n",
    "print(f\"Test Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e60ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 8: Confusion Matrix\n",
    "y_probs = model.predict(x_test / 255.0)\n",
    "y_preds = np.argmax(y_probs, axis=1)\n",
    "y_true = y_test.flatten()\n",
    "cm = confusion_matrix(y_true, y_preds)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names, cmap='Blues')\n",
    "plt.xlabel('Predicted'); plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286cad97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6: Model Evaluation\n",
    "test_loss, test_acc = model.evaluate(test_loader)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}, Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bb33b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 7: Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "y_true = y_test.flatten()\n",
    "conf_mat = confusion_matrix(y_true, predicted_classes)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ae7077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 8: Display Misclassified Images\n",
    "misclassified_indices = np.where(predicted_classes != y_true)[0]\n",
    "plt.figure(figsize=(12, 12))\n",
    "for i, idx in enumerate(misclassified_indices[:9]):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(x_test[idx])\n",
    "    plt.title(f\"True: {class_names[y_true[idx]]}\\nPred: {class_names[predicted_classes[idx]]}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa664e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 9: Classification Report\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(y_true, predicted_classes, target_names=class_names)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6cd8d0",
   "metadata": {},
   "source": [
    "# Task 10: Final Summary\n",
    "透過本次任務，我們完整地建立並評估了一個 CNN 模型於 CIFAR-10 資料集上的表現。\n",
    "\n",
    "- 模型結構包含多層卷積與 Dropout，提高泛化能力\n",
    "- 搭配數據增強提升模型的資料適應力\n",
    "- 最佳測試準確率超過 70%，可視化訓練過程與預測結果\n",
    "- 顯示了錯誤分類與混淆矩陣，有助後續進一步優化\n",
    "\n",
    "未來建議可嘗試引入預訓練模型（如 MobileNet、ResNet）以進一步提升表現。"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
